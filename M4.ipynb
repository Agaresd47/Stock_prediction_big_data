{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae96c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Stock Price Forecast\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.functions import col, isnan, when, count, lit, lead, lag, avg, stddev, min, max, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import year, col, min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "from pyspark.sql.functions import sqrt, avg, lead, col, expr, to_date, year, mean, log, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import  VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.sql.functions import abs as spark_abs\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, FMRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "def scale_features_improved(train_df, val_df, test_df, numerical_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Scale numerical features while properly handling categorical features\n",
    "\n",
    "    Args:\n",
    "        train_df, val_df, test_df: DataFrames for each dataset\n",
    "        numerical_cols: List of numerical feature column names\n",
    "        categorical_cols: List of categorical feature column names\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (scaled_train_df, scaled_val_df, scaled_test_df)\n",
    "    \"\"\"\n",
    "    # 1. Process numerical features\n",
    "    num_assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features_vec\")\n",
    "\n",
    "    # Apply assembler to each dataset\n",
    "    train_num = num_assembler.transform(train_df)\n",
    "    val_num = num_assembler.transform(val_df)\n",
    "    test_num = num_assembler.transform(test_df)\n",
    "\n",
    "    # Fit scaler on training data only\n",
    "    num_scaler = StandardScaler(inputCol=\"numerical_features_vec\", outputCol=\"scaled_numerical_features\")\n",
    "    scaler_model = num_scaler.fit(train_num)\n",
    "\n",
    "    # Transform all datasets\n",
    "    train_scaled = scaler_model.transform(train_num)\n",
    "    val_scaled = scaler_model.transform(val_num)\n",
    "    test_scaled = scaler_model.transform(test_num)\n",
    "\n",
    "    # 2. Process categorical features (assuming they're already indexed/encoded)\n",
    "    cat_assembler = VectorAssembler(inputCols=categorical_cols, outputCol=\"categorical_features_vec\")\n",
    "\n",
    "    # Apply to each dataset\n",
    "    train_final = cat_assembler.transform(train_scaled)\n",
    "    val_final = cat_assembler.transform(val_scaled)\n",
    "    test_final = cat_assembler.transform(test_scaled)\n",
    "\n",
    "    # 3. Combine numerical and categorical features\n",
    "    final_assembler = VectorAssembler(\n",
    "        inputCols=[\"scaled_numerical_features\", \"categorical_features_vec\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    train_result = final_assembler.transform(train_final)\n",
    "    val_result = final_assembler.transform(val_final)\n",
    "    test_result = final_assembler.transform(test_final)\n",
    "\n",
    "    return train_result, val_result, test_result\n",
    "\n",
    "\n",
    "def prepare_pca_for_all_folds(input_dir, output_dir, fold_indices, label_col=\"Target_Return_1d\", pca_k=20):\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    from pyspark.sql.functions import col\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Fitting scaler and PCA using fold 6\")\n",
    "    train_6 = spark.read.parquet(os.path.join(input_dir, \"train_fold_6\"))\n",
    "    price_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adjusted Close\"]\n",
    "    derived_cols = [f.name for f in train_6.schema.fields if isinstance(f.dataType, (DoubleType, IntegerType)) and f.name not in price_cols and not f.name.startswith(\"Target_\")]\n",
    "    numerical_cols = price_cols + derived_cols\n",
    "    categorical_cols = [f.name for f in train_6.schema.fields if f.name.endswith(\"_vec\")]\n",
    "\n",
    "    df_6_scaled, _, _ = scale_features_improved(train_6, train_6, train_6, numerical_cols, categorical_cols)\n",
    "    pca = PCA(inputCol=\"features\", outputCol=\"pca_features\", k=pca_k)\n",
    "    pca_model = pca.fit(df_6_scaled)\n",
    "\n",
    "    def transform_and_save(fold_index):\n",
    "        print(f\"Processing fold {fold_index}\")\n",
    "        df_train = spark.read.parquet(os.path.join(input_dir, f\"train_fold_{fold_index}\"))\n",
    "        df_scaled, _, _ = scale_features_improved(df_train, df_train, df_train, numerical_cols, categorical_cols)\n",
    "        df_pca = pca_model.transform(df_scaled)\n",
    "\n",
    "        df_clean = df_pca.select(\"pca_features\", label_col)\n",
    "        df_clean = df_clean.withColumn(\"pca_array\", vector_to_array(\"pca_features\"))\n",
    "        for i in range(pca_k):\n",
    "            df_clean = df_clean.withColumn(f\"pca_feature_{i}\", col(\"pca_array\")[i])\n",
    "        df_clean = df_clean.drop(\"pca_features\", \"pca_array\")\n",
    "\n",
    "        save_path = os.path.join(output_dir, f\"train_fold_{fold_index}_pca.parquet\")\n",
    "        df_clean.write.mode(\"overwrite\").parquet(save_path)\n",
    "\n",
    "    for i in fold_indices:\n",
    "        transform_and_save(i)\n",
    "\n",
    "    print(\"Processing test set\")\n",
    "    df_test = spark.read.parquet(os.path.join(input_dir, \"test_set\"))\n",
    "    df_test_scaled, _, _ = scale_features_improved(df_test, df_test, df_test, numerical_cols, categorical_cols)\n",
    "    df_test_pca = pca_model.transform(df_test_scaled)\n",
    "\n",
    "    df_test_clean = df_test_pca.select(\"pca_features\", label_col)\n",
    "    df_test_clean = df_test_clean.withColumn(\"pca_array\", vector_to_array(\"pca_features\"))\n",
    "    for i in range(pca_k):\n",
    "        df_test_clean = df_test_clean.withColumn(f\"pca_feature_{i}\", col(\"pca_array\")[i])\n",
    "    df_test_clean = df_test_clean.drop(\"pca_features\", \"pca_array\")\n",
    "    df_test_clean.write.mode(\"overwrite\").parquet(os.path.join(output_dir, \"test_set_pca.parquet\"))\n",
    "\n",
    "    print(\"All folds processed and saved.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eefe7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler and PCA using fold 6\n",
      "Processing fold 0\n",
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n",
      "Processing fold 5\n",
      "Processing fold 6\n",
      "Processing test set\n",
      "All folds processed and saved.\n"
     ]
    }
   ],
   "source": [
    "input_dir = r\"D:\\bigdata_project\\sliding_time_splits_nasdaq\"\n",
    "output_dir = r\"D:\\bigdata_project\\sliding_time_splits_nasdaq_clean\"\n",
    "fold_indices = list(range(7))  # [0, 1, 2, 3, 4, 5, 6]\n",
    "label_col = \"Target_Return_1d\"\n",
    "pca_k = 20\n",
    "\n",
    "prepare_pca_for_all_folds(\n",
    "    input_dir=input_dir,\n",
    "    output_dir=output_dir,\n",
    "    fold_indices=fold_indices,\n",
    "    label_col=label_col,\n",
    "    pca_k=pca_k\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fda4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.ml.regression import LinearRegression, FMRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def load_pca_folds(input_dir, fold_indices, label_col=\"Target_Return_1d\"):\n",
    "    train_folds = []\n",
    "    for i in fold_indices:\n",
    "        df = spark.read.parquet(os.path.join(input_dir, f\"train_fold_{i}_pca.parquet\"))\n",
    "        train_folds.append(df)\n",
    "\n",
    "    test_df = spark.read.parquet(os.path.join(input_dir, \"test_set_pca.parquet\"))\n",
    "    return train_folds, test_df\n",
    "\n",
    "def train_models_on_folds(train_folds, label_col=\"Target_Return_1d\"):\n",
    "    all_models = []\n",
    "    for i, df in enumerate(train_folds):\n",
    "        print(f\"Training models for fold {i}\")\n",
    "\n",
    "        feature_cols = [f\"pca_feature_{j}\" for j in range(20)]\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"pca_features\")\n",
    "        df = assembler.transform(df)\n",
    "\n",
    "        lr_model = LinearRegression(featuresCol=\"pca_features\", labelCol=label_col, regParam=0.01, elasticNetParam=0.0, maxIter=10).fit(df)\n",
    "        print(f\"Training models for fold {i} LR model\")\n",
    "        fm_model = FMRegressor(featuresCol=\"pca_features\", labelCol=label_col, stepSize=0.001, regParam=0.1, maxIter=100).fit(df)\n",
    "        print(f\"Training models for fold {i} FM model\")\n",
    "\n",
    "        all_models.append((lr_model, fm_model))\n",
    "    return all_models\n",
    "def ensemble_predict(models, test_df, label_col=\"Target_Return_1d\"):\n",
    "    feature_cols = [f\"pca_feature_{j}\" for j in range(20)]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"pca_features\")\n",
    "    test_df = assembler.transform(test_df)\n",
    "\n",
    "    preds = []\n",
    "    weights = [i + 1 for i in range(len(models))]  # Recency-weighted: fold 0 = 1, fold 6 = 7\n",
    "    total_weight = sum(weights)\n",
    "\n",
    "    for i, (lr_model, fm_model) in enumerate(models):\n",
    "        weight = weights[i] / total_weight\n",
    "\n",
    "        lr_pred = lr_model.transform(test_df).select(col(\"prediction\").alias(\"lr_pred\"))\n",
    "        fm_pred = fm_model.transform(test_df).select(col(\"prediction\").alias(\"fm_pred\"))\n",
    "\n",
    "        combined = lr_pred.withColumn(\"fm_pred\", fm_pred[\"fm_pred\"])\n",
    "        combined = combined.withColumn(\"weighted_avg\", (col(\"lr_pred\") + col(\"fm_pred\")) / 2 * lit(weight))\n",
    "\n",
    "        preds.append(combined.select(\"weighted_avg\"))\n",
    "\n",
    "    # Sum all weighted predictions\n",
    "    from functools import reduce\n",
    "    from pyspark.sql import DataFrame\n",
    "    combined_preds = reduce(lambda df1, df2: df1.withColumn(\"weighted_avg\", col(\"weighted_avg\") + df2[\"weighted_avg\"]), preds)\n",
    "\n",
    "    final_df = test_df.select(label_col).withColumn(\"ensemble_pred\", combined_preds[\"weighted_avg\"])\n",
    "\n",
    "    evaluators = {\n",
    "        \"rmse\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"rmse\"),\n",
    "        \"mae\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"mae\"),\n",
    "        \"mse\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"mse\"),\n",
    "        \"r2\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"r2\"),\n",
    "    }\n",
    "\n",
    "    for name, evaluator in evaluators.items():\n",
    "        score = evaluator.evaluate(final_df)\n",
    "        print(f\"Final Ensemble {name.upper()}: {score:.6f}\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "035259e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models for fold 0\n",
      "Training models for fold 1\n",
      "Training models for fold 2\n",
      "Training models for fold 3\n",
      "Training models for fold 4\n",
      "Training models for fold 5\n",
      "Training models for fold 6\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"sliding_time_splits_forbes2000_clean\"\n",
    "fold_indices = list(range(7))  # 0 to 6\n",
    "label_col = \"Target_Return_1d\"\n",
    "train_folds, test_df = load_pca_folds(output_dir, fold_indices, label_col)\n",
    "models_per_fold = train_models_on_folds(train_folds, label_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358f4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, test_df, label_col=\"Target_Return_1d\"):\n",
    "    feature_cols = [f\"pca_feature_{j}\" for j in range(20)]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"pca_features\")\n",
    "    test_df = assembler.transform(test_df).withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "    preds = []\n",
    "    weights = [i + 1 for i in range(len(models))]  # Recency-weighted: fold 0 = 1, fold 6 = 7\n",
    "    total_weight = sum(weights)\n",
    "\n",
    "    for i, (lr_model, fm_model) in enumerate(models):\n",
    "        weight = weights[i] / total_weight\n",
    "\n",
    "        lr_pred = lr_model.transform(test_df).select(\"row_id\", col(\"prediction\").alias(\"lr_pred\"))\n",
    "        fm_pred = fm_model.transform(test_df).select(\"row_id\", col(\"prediction\").alias(\"fm_pred\"))\n",
    "\n",
    "        combined = lr_pred.join(fm_pred, on=\"row_id\")\n",
    "        combined = combined.withColumn(f\"weighted_{i}\", ((col(\"lr_pred\") + col(\"fm_pred\")) / 2) * lit(weight))\n",
    "\n",
    "        preds.append(combined.select(\"row_id\", f\"weighted_{i}\"))\n",
    "\n",
    "    from functools import reduce\n",
    "    from pyspark.sql import DataFrame\n",
    "    combined_preds = reduce(lambda df1, df2: df1.join(df2, on=\"row_id\"), preds)\n",
    "\n",
    "    weight_cols = [f\"weighted_{i}\" for i in range(len(models))]\n",
    "    combined_preds = combined_preds.withColumn(\"ensemble_pred\", sum([col(c) for c in weight_cols]))\n",
    "\n",
    "    final_df = test_df.select(\"row_id\", label_col).join(combined_preds.select(\"row_id\", \"ensemble_pred\"), on=\"row_id\")\n",
    "    final_df = final_df.select(label_col, \"ensemble_pred\")\n",
    "\n",
    "    evaluators = {\n",
    "        \"rmse\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"rmse\"),\n",
    "        \"mae\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"mae\"),\n",
    "        \"mse\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"mse\"),\n",
    "        \"r2\": RegressionEvaluator(labelCol=label_col, predictionCol=\"ensemble_pred\", metricName=\"r2\"),\n",
    "    }\n",
    "\n",
    "    for name, evaluator in evaluators.items():\n",
    "        score = evaluator.evaluate(final_df)\n",
    "        print(f\"Final Ensemble {name.upper()}: {score:.6f}\")\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for dataset: sliding_time_splits_forbes2000_clean\n",
      "Final Ensemble RMSE: 0.048952\n",
      "Final Ensemble MAE: 0.015401\n",
      "Final Ensemble MSE: 0.002542\n",
      "Final Ensemble R2: -0.013567\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b51f821",
   "metadata": {},
   "source": [
    "## Nasdaq dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b0a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models for fold 0\n",
      "Training models for fold 1\n",
      "Training models for fold 2\n",
      "Training models for fold 3\n",
      "Training models for fold 4\n",
      "Training models for fold 5\n",
      "Training models for fold 6\n",
      "Final Ensemble RMSE: 0.065031\n",
      "Final Ensemble MAE: 0.022865\n",
      "Final Ensemble MSE: 0.004225\n",
      "Final Ensemble R2: 0.015129\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"sliding_time_splits_nasdaq_clean\"\n",
    "fold_indices = list(range(7))  # 0 to 6\n",
    "label_col = \"Target_Return_1d\"\n",
    "train_folds, test_df = load_pca_folds(output_dir, fold_indices, label_col)\n",
    "models_per_fold = train_models_on_folds(train_folds, label_col)\n",
    "final_df = ensemble_predict(models_per_fold, test_df, label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c251e0",
   "metadata": {},
   "source": [
    "## Forbes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models for fold 0\n",
      "Training models for fold 1\n",
      "Training models for fold 2\n",
      "Training models for fold 3\n",
      "Training models for fold 4\n",
      "Training models for fold 5\n",
      "Training models for fold 6\n",
      "Final Ensemble RMSE: 0.078433\n",
      "Final Ensemble MAE: 0.027189\n",
      "Final Ensemble MSE: 0.008305\n",
      "Final Ensemble R2: 0.017433\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"sliding_time_splits_forbes2000_clean\"\n",
    "fold_indices = list(range(7))  # 0 to 6\n",
    "label_col = \"Target_Return_1d\"\n",
    "train_folds, test_df = load_pca_folds(output_dir, fold_indices, label_col)\n",
    "models_per_fold = train_models_on_folds(train_folds, label_col)\n",
    "final_df = ensemble_predict(models_per_fold, test_df, label_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
