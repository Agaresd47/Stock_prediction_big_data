# ğŸ“ˆ Stock Return Prediction with PySpark

## ğŸ“Œ Overview

This project focuses on predicting one-day-ahead stock returns (`Target_Return_1d`) using historical financial time series data. We employ a scalable pipeline powered by PySpark to preprocess, engineer features, perform dimensionality reduction, and train predictive models using sliding window strategies.

## â“ Statement of the Problem

Accurately forecasting short-term stock returns is notoriously difficult due to market noise and high volatility. Our goal is to build a robust, scalable, and interpretable model to predict `Target_Return_1d` across multiple stocks, using engineered features and time-aware cross-validation.

## ğŸ“‚ Data Sources and Description
- **Data Sources**:  
- [Kaggle Stock Market Data](https://www.kaggle.com/datasets/paultimothymooney/stock-market-data)
- **Datasets**:  
  - `sliding_time_splits_forbes2000`  
  - `sliding_time_splits_nasdaq`  
  

- **Data Format**:  
  - CSV files: Daily prices (`Open`, `High`, `Low`, `Close`, `Volume`, `Adjusted Close`)  
  - JSON files: Metadata such as currency, instrument type, and exchange info

## ğŸ—‚ï¸ Repository Structure

```
â”œâ”€â”€ data/                  # Raw and checkpointed parquet datasets
â”œâ”€â”€ notebooks/             # Exploratory data analysis and prototype modeling
â”œâ”€â”€ src/                   # PySpark pipeline scripts and modules
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ model_training.py
â”œâ”€â”€ output/                # Model outputs and evaluation metrics
â”œâ”€â”€ config/                # Parameters and hyperparameter grids
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## âš™ï¸ Approach

We followed a modular pipeline architecture using PySpark:
- Preprocessing with schema enforcement and outlier detection
- Feature engineering (returns, volatility, lag features)
- PCA for dimensionality reduction
- Sliding-window cross-validation (5y train / 1y val / 1y test)
- Modeling with `FMRegressor` and `LinearRegression`, ensembled using time-weighted folds

**Core PySpark Tools Used**:
- `VectorAssembler`, `StandardScaler`, `StringIndexer`, `OneHotEncoder`
- `PCA`, `FMRegressor`, `CrossValidator`, `RegressionEvaluator`

**Parquet checkpointing** was used at preprocessing and fold generation stages to improve reproducibility and reduce recomputation.

## ğŸ—“ï¸ Timeline / Deliverables

- âœ… Week 1â€“2: Data exploration, schema standardization, outlier detection
- âœ… Week 3â€“4: Feature engineering, return/volatility calculations
- âœ… Week 5: Sliding window fold generation and preprocessing checkpointing
- âœ… Week 6â€“7: PCA and model training (FMRegressor + Linear Regression)
- âœ… Week 8: Recency-weighted ensemble, evaluation, and report writing

## ğŸ“š Resources

- [PySpark MLlib Documentation](https://spark.apache.org/docs/latest/ml-guide.html)
- Kaggle Stock Dataset: https://www.kaggle.com/datasets/paultimothymooney/stock-market-data

## ğŸ¤ How to Contribute

This is currently a closed academic project. If you're interested in contributing or adapting this pipeline, feel free to open an issue or contact the authors.

## â–¶ï¸ Sample Run / Output

_(To be added...)_
```

---

Let me know once you're ready to plug in sample output, or if you'd like this converted to `.rst` or `.txt` format for broader usage.